{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Bài 1:"
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# SỬA Ở ĐÂY: Thêm tham số model=\"bert-base-uncased\" hoặc \"distilbert-base-uncased\"\n",
    "mask_filler = pipeline(\"fill-mask\", model=\"bert-base-uncased\")\n",
    "\n",
    "# Giữ nguyên [MASK] đúng chuẩn BERT\n",
    "input_sentence = \"Hanoi is the [MASK] of Vietnam.\"\n",
    "\n",
    "predictions = mask_filler(input_sentence, top_k=5)\n",
    "\n",
    "print(f\"Câu gốc: {input_sentence}\")\n",
    "for pred in predictions:\n",
    "    print(f\"Dự đoán: '{pred['token_str']}' -> Câu: {pred['sequence']}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2Zy4ztAf9LYW",
    "outputId": "c52b0759-b76a-43c7-e663-f114ddea0d4f"
   },
   "execution_count": 1,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Câu gốc: Hanoi is the [MASK] of Vietnam.\n",
      "Dự đoán: 'capital' -> Câu: hanoi is the capital of vietnam.\n",
      "Dự đoán: 'center' -> Câu: hanoi is the center of vietnam.\n",
      "Dự đoán: 'birthplace' -> Câu: hanoi is the birthplace of vietnam.\n",
      "Dự đoán: 'headquarters' -> Câu: hanoi is the headquarters of vietnam.\n",
      "Dự đoán: 'city' -> Câu: hanoi is the city of vietnam.\n"
     ]
    }
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Mô hình bert-base-uncased đã dự đoán từ \"capital\" với độ tin cậy cao nhất (xếp vị trí đầu tiên trong danh sách kết quả).\n",
    "#### Lý do vì sao các mô hình Encoder-only như BERT lại phù hợp cho tác vụ này:\n",
    "\n",
    "* Mô hình Encoder-only (như BERT) sử dụng cơ chế Attention hai chiều (Bidirectional Attention).\n",
    "\n",
    "* Để điền đúng từ vào [MASK], mô hình cần nhìn thấy cả ngữ cảnh bên trái (\"Hanoi is the...\") và bên phải (\"...of Vietnam\").\n",
    "\n",
    "* Kiến trúc Encoder cho phép mô hình \"đọc\" toàn bộ câu cùng một lúc để hiểu trọn vẹn ngữ nghĩa, từ đó đưa ra dự đoán chính xác nhất. Nếu chỉ đọc một chiều (từ trái sang phải), mô hình có thể gặp khó khăn vì chưa biết đối tượng phía sau là \"Vietnam\"."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Bài 2:"
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import pipeline, set_seed\n",
    "\n",
    "# Đặt seed để kết quả có thể tái lập (tùy chọn)\n",
    "set_seed(42)\n",
    "\n",
    "# 1. Tải pipeline \"text-generation\" (thường dùng GPT-2)\n",
    "generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "\n",
    "# 2. Đoạn văn bản mồi\n",
    "prompt = \"The best thing about learning NLP is\"\n",
    "\n",
    "# 3. Sinh văn bản\n",
    "# pad_token_id được gán bằng eos_token_id để tránh cảnh báo trong log\n",
    "generated_texts = generator(prompt, max_length=50, num_return_sequences=1, pad_token_id=50256)\n",
    "\n",
    "# 4. In kết quả\n",
    "print(f\"Câu mồi: '{prompt}'\")\n",
    "for text in generated_texts:\n",
    "    print(\"\\nVăn bản được sinh ra:\")\n",
    "    print(text['generated_text'])"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P9ro7Mv19sFo",
    "outputId": "f7c0607f-9589-4507-c1cc-672434578078"
   },
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Device set to use cuda:0\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Câu mồi: 'The best thing about learning NLP is'\n",
      "\n",
      "Văn bản được sinh ra:\n",
      "The best thing about learning NLP is that you learn to learn something, and you learn it through hard work and hard work. It's not like you learn all at once, but you learn at what is most important.\n",
      "\n",
      "If you want to learn more, there are a lot of books out there that are written about NLP, and I think that's why they make sense. But at the same time, there are many books that have been written about NLP that people have done that have actually been successful.\n",
      "\n",
      "JUAN GONZÁLEZ: Well, this is a little bit of an aside. On a personal note, I'm really curious to know what the people who have been on the frontlines of NLP are doing right now. I know there's an organization called the NLP Project, and they're making a lot of changes going forward, and I'm curious about how they're doing with the NLP Project.\n",
      "\n",
      "So, the NLP Project is a group of people who are doing about two things right now. One is trying to get the word out about NLP in the press. The other is trying to get the word out about the NLP Project, and then building up to a broader audience. So, I guess, they\n"
     ]
    }
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Kết quả sinh ra có hợp lý không?\n",
    "\n",
    "* Về mặt ngữ pháp: Kết quả rất trôi chảy và đúng ngữ pháp tiếng Anh (VD: \"The best thing about learning NLP is that you learn to learn something...\").\n",
    "\n",
    "* Về mặt ngữ nghĩa:\n",
    "\n",
    "    * Phần đầu khá hợp lý và liên quan đến chủ đề học tập/sách vở.\n",
    "\n",
    "    * Phần sau bị lạc đề (hallucination). Mô hình tự động chuyển sang định dạng kịch bản phỏng vấn với nhân vật giả định (\"JUAN GONZÁLEZ\") và một tổ chức không rõ ràng (\"NLP Project\").\n",
    "\n",
    "##### Kết luận: Mô hình tốt ở việc tạo câu đúng cú pháp nhưng thiếu khả năng duy trì mạch tư duy logic dài\n",
    "\n",
    "#### Vì sao các mô hình Decoder-only như GPT lại phù hợp cho tác vụ này?\n",
    "\n",
    "* Mô hình Decoder-only hoạt động theo cơ chế Tự hồi quy (Autoregressive).\n",
    "\n",
    "* Nó sử dụng Causal Masking (Che giấu nhân quả), tức là tại thời điểm dự đoán từ hiện tại, nó chỉ được phép nhìn thấy các từ trong quá khứ (bên trái) và bị che hoàn toàn các từ tương lai (bên phải).\n",
    "\n",
    "* Cơ chế này mô phỏng chính xác cách con người viết văn bản tự nhiên (viết từng từ nối tiếp nhau từ trái sang phải), do đó nó tối ưu cho tác vụ sinh văn bản (Text Generation/Next Token Prediction)."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Bài 3:"
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# 1. Chọn một mô hình BERT\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# 2. Câu đầu vào\n",
    "sentences = [\"This is a sample sentence.\"]\n",
    "\n",
    "# 3. Tokenize câu\n",
    "inputs = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "# 4. Đưa qua mô hình để lấy hidden states\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# last_hidden_state chứa vector đầu ra của tất cả các token\n",
    "# Kích thước: (batch_size, sequence_length, hidden_size)\n",
    "last_hidden_state = outputs.last_hidden_state\n",
    "\n",
    "# 5. Thực hiện Mean Pooling (Tính trung bình cộng có trọng số của mask)\n",
    "attention_mask = inputs['attention_mask']\n",
    "\n",
    "# Mở rộng mask để khớp kích thước với last_hidden_state\n",
    "mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "\n",
    "# Tính tổng các vector (chỉ những token thật, bỏ qua padding)\n",
    "sum_embeddings = torch.sum(last_hidden_state * mask_expanded, 1)\n",
    "\n",
    "# Tính tổng số lượng token thật (tránh chia cho 0 bằng cách kẹp min=1e-9)\n",
    "sum_mask = torch.clamp(mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "# Chia tổng cho số lượng để ra trung bình\n",
    "sentence_embedding = sum_embeddings / sum_mask\n",
    "\n",
    "# 6. In kết quả\n",
    "print(\"Vector biểu diễn của câu (5 giá trị đầu):\")\n",
    "print(sentence_embedding[0][:5]) # Chỉ in 5 số đầu để gọn\n",
    "print(\"\\nKích thước của vector:\", sentence_embedding.shape)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yeReUkwx95rJ",
    "outputId": "81fde428-594d-437d-dfc2-294c6f313e9e"
   },
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Vector biểu diễn của câu (5 giá trị đầu):\n",
      "tensor([-0.0639, -0.4284, -0.0668, -0.3843, -0.0658])\n",
      "\n",
      "Kích thước của vector: torch.Size([1, 768])\n"
     ]
    }
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Kích thước của vector biểu diễn là 768. Con số này tương ứng với tham số hidden_size (kích thước lớp ẩn) của kiến trúc bert-base.\n",
    "\n",
    "#### Lý do cần sử dụng attention_mask khi thực hiện Mean Pooling:\n",
    "\n",
    "* Để đảm bảo tính trung bình cộng phản ánh đúng nội dung câu, chúng ta cần loại bỏ các token đệm (padding tokens).\n",
    "\n",
    "* Trong xử lý theo batch, các câu ngắn được thêm các số 0 (padding) để bằng độ dài câu dài nhất. Nếu tính trung bình cộng tất cả (bao gồm cả các số 0 này), giá trị vector sẽ bị chia nhỏ đi và làm lệch lạc ngữ nghĩa của câu.\n",
    "\n",
    "* attention_mask giúp đánh dấu đâu là từ thật (giá trị 1), đâu là đệm (giá trị 0). Phép toán sum_embeddings / sum_mask đảm bảo ta chỉ chia tổng vector cho số lượng từ thực tế có trong câu."
   ]
  }
 ]
}
